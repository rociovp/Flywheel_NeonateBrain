{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download files from Flywheel\n",
    "# Written by David Parker - updated Oct 2022\n",
    "# Adapted by Lisa Bruckert Nov 2021\n",
    "# Adapted by Rocio Poblaciones Apr 2022\n",
    "# Refactor for use of pipeline csv file Dec 2022\n",
    "\n",
    "\"\"\"\n",
    "Changelog:\n",
    "10/31/2022 - Parker\n",
    " - Added regular expression filter ability to analysis label\n",
    " - Added gear version filter to analysis search\n",
    " - reorganized code/ added blocks\n",
    " - removed API key references.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import flywheel\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pathvalidate as pv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "PIPELINE_GEAR_NAME = \"infant-preproc-pipeline\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "System settings to be used through the script"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Create a work directory in our local \"home\" directory\n",
    "# Example: work_dir = Path(Path.home()/'Documents/Flywheel_QCreport', platform='auto')\n",
    "#work_dir = Path('//')\n",
    "work_dir = Path('/Users/davidparker/Travis-Lab/MRI/neonateMRIdata/FWAnalyses/SDKAug2022_b1500/')\n",
    "# If it doesn't exist, create it\n",
    "if not work_dir.exists():\n",
    "    work_dir.mkdir(parents = True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Flywheel settings to be used through the script"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Set the project ID you wish to download from:\n",
    "project_id = '5eafb9fd788701016978097d'\n",
    "\n",
    "# Set the subject labels you wish to download from\n",
    "subject_ids_to_download = ['21346',]\n",
    "\n",
    "# Set the gear we're looking for,\n",
    "# along with the list of files to download for that gear.\n",
    "# Example: gear = 'bids-freesurfer'\n",
    "rtp_pipeline_files=['t1.nii.gz',\n",
    "               'CC_Mot_wbt_noEval_clean.tck',\n",
    "               'CC_Occ_wbt_noEval_clean.tck',\n",
    "               'CC_OrbFron_wbt_noEval_clean.tck',\n",
    "               'CC_PostPar_wbt_noEval_clean.tck',\n",
    "               'CC_SupFron_wbt_noEval_clean.tck',\n",
    "               'CC_SupPar_wbt_noEval_clean.tck',\n",
    "               'CC_Temp_wbt_noEval_clean.tck',\n",
    "               'CC_AntFron_wbt_noEval_clean.tck',\n",
    "               'RTP_fa.csv',\n",
    "               'RTP_ad.csv',\n",
    "               'RTP_rd.csv',\n",
    "                'RTP_md.csv',\n",
    "               'RTP_ad.csv',\n",
    "                'RTP_cl.csv',\n",
    "                'RTP_rd.csv',\n",
    "                'RTP_C2ROIad.csv',\n",
    "                'RTP_C2ROIcl.csv',\n",
    "                'RTP_C2ROImd.csv',\n",
    "                'RTP_C2ROIad.csv',\n",
    "                'RTP_C2ROIrd.csv',\n",
    "                'RTP_C2ROIfa.csv',\n",
    "                'CFMaj_wbt_noEval_clean.tck',\n",
    "                'CFMin_wbt_noEval_clean.tck',\n",
    "                'LAF_wbt_noEval_clean.tck',\n",
    "               'LATR_wbt_noEval_clean.tck',\n",
    "               'LCC_wbt_noEval_clean.tck','LCH_wbt_noEval_clean.tck','LCST_wbt_noEval_clean.tck',\n",
    "               'LICP_wbt_noEval_clean.tck','LIFOF_wbt_noEval_clean.tck','LILF_wbt_noEval_clean.tck',\n",
    "               'LSCP_wbt_noEval_clean.tck','LSLF_wbt_noEval_clean.tck','LUF_wbt_noEval_clean.tck',\n",
    "               'MCP_wbt_noEval_clean.tck','RAF_wbt_noEval_clean.tck','RATR_wbt_noEval_clean.tck',\n",
    "               'RCC_wbt_noEval_clean.tck','RCH_wbt_noEval_clean.tck','RCST_wbt_noEval_clean.tck',\n",
    "               'RICP_wbt_noEval_clean.tck','RIFOF_wbt_noEval_clean.tck', 'RILF_wbt_noEval_clean.tck',\n",
    "               'RSCP_wbt_noEval_clean.tck','RSLF_wbt_noEval_clean.tck',\n",
    "               'RUF_wbt_noEval_clean.tck','dwi.nii.gz','dwi_wmCsd_autolmax.mif']\n",
    "gear_dict = {'rtp-pipeline':rtp_pipeline_files}\n",
    "# Set the regular expression to match analyses to.  This will find the\n",
    "# pipeline gear with an analysis label that matches this regex\n",
    "analysis_label_regex = r\"b1500\"\n",
    "aregex = re.compile(analysis_label_regex)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize flywheel thingies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Project ID for PT_NeonateBrain\n",
    "# I recommend storing your API key as an environment variable and then referencing it this way\n",
    "# You can do this in bash:\n",
    "# export STAN_API=\"<my_api_key>\"\n",
    "# Before running this script.\n",
    "\n",
    "fw = flywheel.Client(os.environ['STAN_API'])\n",
    "project = fw.get_project(project_id)\n",
    "\n",
    "# Create a custom path for our project (we may run this on other projects in the future) and create if it doesn't exist\n",
    "project_path = pv.sanitize_filepath(work_dir/project.label,platform='auto')\n",
    "print(project_path)\n",
    "\n",
    "if not project_path.exists():\n",
    "    project_path.mkdir()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the search and loop over the subjects specified"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can loop over sessions (and skip subjects), because the subject parent info is stored on the session if we need it,\n",
    "# AND the analysis of interest is stored on the session itself.\n",
    "for ses in tqdm(project.sessions.iter()):\n",
    "\n",
    "\n",
    "    # If subjects_ids_to_download is a list with some ids inside\n",
    "    # then lets only do those.\n",
    "    # If subject_ids_to_download is NOT False and ses_label IS inside subject_ids_to_download\n",
    "    # then do the process below\n",
    "    # else, continue\n",
    "\n",
    "    ses_label = ses.label\n",
    "    sub_label = ses.subject.label\n",
    "\n",
    "    if (subject_ids_to_download is not False) & (str(sub_label) not in subject_ids_to_download):\n",
    "        #print(\"Jumping \" + str(ses_label))\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Analyzing Subject id: \" + str(ses_label) + \" Session id: \" + str(sub_label))\n",
    "\n",
    "\n",
    "    # Make sure we have all our analysis since we got the session through an iterator, and not \"fw.get()'\n",
    "    ses = ses.reload()\n",
    "    analyses = ses.analyses\n",
    "\n",
    "    # If there are no analyses containers, we know that this gear was not run. Move on to the next session\n",
    "    if len(analyses) == 0:\n",
    "        continue\n",
    "\n",
    "    # Otherwise there are analyses containers\n",
    "    else:\n",
    "        print(f'{ses.label} has analysis')\n",
    "\n",
    "        # Check to see if any were generated by our gear\n",
    "        matches = [\n",
    "                    asys for asys in analyses if\n",
    "                    asys.gear_info.get('name') == PIPELINE_GEAR_NAME\n",
    "                    and aregex.findall(asys.label)\n",
    "                   ]\n",
    "\n",
    "        print(f'{len(matches)} matches in {[asys.label for asys in analyses]}')\n",
    "        # If there are no matches, the gear didn't run\n",
    "        if len(matches) == 0:\n",
    "            continue\n",
    "\n",
    "        # If there is one match, that's our target\n",
    "        elif len(matches) == 1:\n",
    "            match = matches[0]\n",
    "\n",
    "        # If there are more than one matches (due to reruns), take the most recent run.\n",
    "        # This behavior may be modified to whatever suits your needs\n",
    "        else:\n",
    "\n",
    "           # Loop through the analyses and first make sure we only look at successful runs\n",
    "            matches = [asys for asys in matches if asys.job.get('state')=='complete']\n",
    "            print(f'{len(matches)} completed matches')\n",
    "\n",
    "            # Now find the max run date (most recent), and extract the analysis that has that date.\n",
    "            last_run_date = max([asys.created for asys in matches])\n",
    "            last_run_analysis = [asys for asys in matches if asys.created == last_run_date]\n",
    "\n",
    "            # There should only be one exact match.  If there are two successful runs that happened at the same time,\n",
    "            # Something is strange...just take one at random.\n",
    "            match = last_run_analysis[0]\n",
    "\n",
    "        status = match.job.get('state')\n",
    "\n",
    "        # If the status is complete, look for the output file:\n",
    "        if status == 'complete':\n",
    "            # Put the download section within a \"try\" loop in case there are API errors downloading.\n",
    "            try:\n",
    "                # Reload the match and let's look at the files\n",
    "                # match = match.reload()\n",
    "                # files = match.files\n",
    "\n",
    "                pipeline = match.read_file('pipeline.csv').decode('utf-8')\n",
    "                pipeline = list(csv.reader(pipeline.splitlines()))\n",
    "\n",
    "                for gear in gear_dict:\n",
    "                    job = [row[-1] for row in pipeline if gear in row]\n",
    "\n",
    "                    if not job:\n",
    "                        print('No job found for {}'.format(gear))\n",
    "                        continue\n",
    "\n",
    "                    job=job[0]\n",
    "\n",
    "                    asys=fw.get_analysis(job)\n",
    "                    #asys = fw.get_analysis(job.destination['id'])\n",
    "\n",
    "                    files = asys.files\n",
    "                    file_name_list = gear_dict[gear]\n",
    "                    # In case there are more files (there shouldn't be), find the one that's\n",
    "                    # A zip archive.\n",
    "                    if len(files) > 1:\n",
    "                        files = [f for f in files if f.mimetype == 'application/zip']\n",
    "\n",
    "                    # Exctract the file object\n",
    "                    file = files[0]\n",
    "                    # Get its name\n",
    "                    fname = file.name\n",
    "                    print(\"fname\")\n",
    "                    print(fname)\n",
    "\n",
    "                    # Get the zip members.  We're looking for one particular file called \"aseg.stats\", but the actual\n",
    "                    # Directory may be different from subject to subject, as the parent directories have subject ID's in their name\n",
    "                    zip_info = asys.get_file_zip_info(fname)['members']\n",
    "\n",
    "                    # We'll identify any strings that have this aseg.stats string in them\n",
    "                    # Example: file_of_interest = [a['path'] for a in zip_info if '/aseg.stats' in a['path']]\n",
    "\n",
    "                   #We loop over files using variable file_name that includes all elements in file_name_list\n",
    "\n",
    "                    for file_name in file_name_list:\n",
    "                        file_of_interest = [a['path'] for a in zip_info if '/'+ file_name in a['path']]\n",
    "                        print(file_of_interest)\n",
    "                    # If we found some (There should be one), set that as our file of interest.\n",
    "                        if len(file_of_interest) > 0:\n",
    "                            file_of_interest = file_of_interest[0]\n",
    "                        else:\n",
    "                            print(f'No File of Interest found for {sub_label} {ses_label} {asys.label}' )\n",
    "                            continue\n",
    "\n",
    "\n",
    "                    #We create a variable download_name with var file_name and sanitize name.\n",
    "                        download_name = Path(pv.sanitize_filename(f'{asys.label}'+'_' + file_name),platform='auto')\n",
    "\n",
    "                        download_dir = pv.sanitize_filepath(project_path/sub_label/gear,platform='auto')\n",
    "                        # Create the path\n",
    "                        if not download_dir.exists():\n",
    "                            download_dir.mkdir(parents=True)\n",
    "\n",
    "                        download_path = download_dir/download_name\n",
    "                        print('download_path')\n",
    "                        print(download_path)\n",
    "\n",
    "                    # Download the file\n",
    "                    #If we don't want to replace existing files use if not, if we want to replace them skip this step\n",
    "                        if not os.path.exists(download_path):\n",
    "                            print('downloading file')\n",
    "                            asys.download_file_zip_member(fname, file_of_interest, download_path)\n",
    "                        else:\n",
    "                            print('File exists')\n",
    "\n",
    "\n",
    "            # Alert the user of any exceptions.\n",
    "            except Exception as e:\n",
    "                print('Error Downloading File')\n",
    "                print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
